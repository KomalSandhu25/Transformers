# Transformers
The code for this repo is private.

**Problem statement**: Generate translation from one language to another.

**The code include**:

1. Implementing the encoder part for the transformer based on the paper "Attenion is all you need".
2. Encoder consists of an embedding layer, self attention layer, linear layer and the softmax layer to compute the output probabilities. The implementation is done from scratch.
3. Next is implementing the the full tranformer encoder-decoder layer using Pytorch's buil-in Transformer function.
